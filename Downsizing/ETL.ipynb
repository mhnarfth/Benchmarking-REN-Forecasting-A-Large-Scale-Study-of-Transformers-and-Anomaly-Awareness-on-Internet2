{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "776757be-f31c-4889-b753-b9097f3e7bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete.\n",
      "Configuration loaded.\n",
      "Input combined raw data: /mnt/nrdstor/ramamurthy/mhnarfth/internet2/parquet/combined/all_routers_combined.parquet_dataset\n",
      "Outputs will be saved to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4 (figs: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/figs)\n",
      "Processed individual hourly files to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/processed_hourly_individual\n",
      "Final combined hourly dataset to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/all_routers_hourly_processed_with_anomalies.parquet_dataset\n",
      "Anomaly Detection contamination (initial): 0.01\n",
      "\n",
      "--- Initiating Notebook 1: Data Ingestion, Preprocessing, and EDA (Multi-Router) ---\n",
      "\n",
      "Loading combined raw data from: /mnt/nrdstor/ramamurthy/mhnarfth/internet2/parquet/combined/all_routers_combined.parquet_dataset\n",
      "Loaded Dask DataFrame (lazy): 55 partitions.\n",
      "Found 10 unique routers: ['atlanta', 'batonrouge', 'boston', 'dallas', 'elpaso', 'jackson', 'jacksonville', 'louisville', 'phoenix', 'reno']\n",
      "\n",
      "Collecting 0.05% of total raw data for Fig 4-2c (Flow-size Distribution)...\n",
      "  Collected 173934 raw flow samples for global histogram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Starting Full Processing for Router: atlanta (1/10) ====\n",
      "  Raw data for atlanta: 28032240 records.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for atlanta...\n",
      "  Computed hourly data for atlanta: 1369 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for atlanta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  10%|█         | 1/10 [03:35<32:16, 215.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for atlanta. Found 13 anomalies.\n",
      "  Saving processed hourly data for atlanta to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/processed_hourly_individual/hourly_atlanta_processed_with_anomalies.parquet\n",
      "  Processed hourly data for atlanta saved successfully.\n",
      "  Skipping single-router EDA plots for atlanta (generating for elpaso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: batonrouge (2/10) ====\n",
      "  Raw data for batonrouge: 80056602 records.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for batonrouge...\n",
      "  Computed hourly data for batonrouge: 1343 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for batonrouge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  20%|██        | 2/10 [08:20<34:11, 256.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for batonrouge. Found 14 anomalies.\n",
      "  Saving processed hourly data for batonrouge to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/processed_hourly_individual/hourly_batonrouge_processed_with_anomalies.parquet\n",
      "  Processed hourly data for batonrouge saved successfully.\n",
      "  Skipping single-router EDA plots for batonrouge (generating for elpaso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: boston (3/10) ====\n",
      "  Raw data for boston: 392891 records.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for boston...\n",
      "  Computed hourly data for boston: 789 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for boston...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  30%|███       | 3/10 [11:27<26:14, 224.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for boston. Found 8 anomalies.\n",
      "  Saving processed hourly data for boston to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/processed_hourly_individual/hourly_boston_processed_with_anomalies.parquet\n",
      "  Processed hourly data for boston saved successfully.\n",
      "  Skipping single-router EDA plots for boston (generating for elpaso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: dallas (4/10) ====\n",
      "  Raw data for dallas: 149784626 records.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for dallas...\n",
      "  Computed hourly data for dallas: 1322 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for dallas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  40%|████      | 4/10 [17:14<27:17, 272.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for dallas. Found 14 anomalies.\n",
      "  Saving processed hourly data for dallas to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/processed_hourly_individual/hourly_dallas_processed_with_anomalies.parquet\n",
      "  Processed hourly data for dallas saved successfully.\n",
      "  Skipping single-router EDA plots for dallas (generating for elpaso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: elpaso (5/10) ====\n",
      "  Raw data for elpaso: 9357137 records.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for elpaso...\n",
      "  Computed hourly data for elpaso: 1378 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for elpaso...\n",
      "  Isolation Forest detection complete for elpaso. Found 14 anomalies.\n",
      "  Saving processed hourly data for elpaso to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/processed_hourly_individual/hourly_elpaso_processed_with_anomalies.parquet\n",
      "  Processed hourly data for elpaso saved successfully.\n",
      "\n",
      "--- Generating Single-Router EDA Plots for elpaso ---\n",
      "  Generating Figure 4-2b (Week-of-Day Heatmap for elpaso)...\n",
      "  Saved 4_2b_weekday_heatmap_elpaso.png and 4_2b_weekday_heatmap_elpaso.pdf\n",
      "\n",
      "--- Generating Anomaly Detection Plots for elpaso ---\n",
      "  Saved 4_3a_if_score_histogram_elpaso.png and 4_3a_if_score_histogram_elpaso.pdf\n",
      "  Saved 4_3b_timeseries_anomalies_elpaso.png and 4_3b_timeseries_anomalies_elpaso.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  50%|█████     | 5/10 [20:26<20:19, 243.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 4_3c_anomaly_counts_by_day_elpaso.png and 4_3c_anomaly_counts_by_day_elpaso.pdf\n",
      "\n",
      "==== Starting Full Processing for Router: jackson (6/10) ====\n",
      "  Raw data for jackson: 13401496 records.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for jackson...\n",
      "  Computed hourly data for jackson: 1369 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for jackson...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  60%|██████    | 6/10 [23:41<15:08, 227.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for jackson. Found 14 anomalies.\n",
      "  Saving processed hourly data for jackson to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/processed_hourly_individual/hourly_jackson_processed_with_anomalies.parquet\n",
      "  Processed hourly data for jackson saved successfully.\n",
      "  Skipping single-router EDA plots for jackson (generating for elpaso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: jacksonville (7/10) ====\n",
      "  Raw data for jacksonville: 19425447 records.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for jacksonville...\n",
      "  Computed hourly data for jacksonville: 1338 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for jacksonville...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  70%|███████   | 7/10 [27:03<10:56, 218.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for jacksonville. Found 14 anomalies.\n",
      "  Saving processed hourly data for jacksonville to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/processed_hourly_individual/hourly_jacksonville_processed_with_anomalies.parquet\n",
      "  Processed hourly data for jacksonville saved successfully.\n",
      "  Skipping single-router EDA plots for jacksonville (generating for elpaso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: louisville (8/10) ====\n",
      "  Raw data for louisville: 3150312 records.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for louisville...\n",
      "  Computed hourly data for louisville: 1466 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for louisville...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  80%|████████  | 8/10 [30:04<06:53, 206.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for louisville. Found 15 anomalies.\n",
      "  Saving processed hourly data for louisville to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/processed_hourly_individual/hourly_louisville_processed_with_anomalies.parquet\n",
      "  Processed hourly data for louisville saved successfully.\n",
      "  Skipping single-router EDA plots for louisville (generating for elpaso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: phoenix (9/10) ====\n",
      "  Raw data for phoenix: 8765751 records.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for phoenix...\n",
      "  Computed hourly data for phoenix: 1369 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for phoenix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  90%|█████████ | 9/10 [33:21<03:23, 203.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for phoenix. Found 14 anomalies.\n",
      "  Saving processed hourly data for phoenix to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/processed_hourly_individual/hourly_phoenix_processed_with_anomalies.parquet\n",
      "  Processed hourly data for phoenix saved successfully.\n",
      "  Skipping single-router EDA plots for phoenix (generating for elpaso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: reno (10/10) ====\n",
      "  Raw data for reno: 35499401 records.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for reno...\n",
      "  Computed hourly data for reno: 861 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for reno...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers: 100%|██████████| 10/10 [37:07<00:00, 222.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for reno. Found 9 anomalies.\n",
      "  Saving processed hourly data for reno to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/processed_hourly_individual/hourly_reno_processed_with_anomalies.parquet\n",
      "  Processed hourly data for reno saved successfully.\n",
      "  Skipping single-router EDA plots for reno (generating for elpaso as representative).\n",
      "\n",
      "==== All Routers Processed. Generating Global EDA Plots ====\n",
      "  No hourly mean data collected. Skipping Fig 4-2a.\n",
      "\n",
      "--- Generating Global EDA Plots (Figures 4-2a, 4-2c) ---\n",
      "  No hourly mean data collected. Skipping Fig 4-2a.\n",
      "  Generating Figure 4-2c (Flow Size Distribution - All Routers Sample)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 4_2c_flow_size_histogram_all_routers.png and 4_2c_flow_size_histogram_all_routers.pdf\n",
      "\n",
      "--- Table 4-1: Router Inventory (Raw Data & Hourly Aggregates) ---\n",
      "      Router ID  Raw Flows (#)  Hourly Rows (#)\n",
      "0       atlanta       28032240             1369\n",
      "1    batonrouge       80056602             1343\n",
      "2        boston         392891              789\n",
      "3        dallas      149784626             1322\n",
      "4        elpaso        9357137             1378\n",
      "5       jackson       13401496             1369\n",
      "6  jacksonville       19425447             1338\n",
      "7    louisville        3150312             1466\n",
      "8       phoenix        8765751             1369\n",
      "9          reno       35499401              861\n",
      "\n",
      "--- Consolidating all individual hourly processed files to: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/all_routers_hourly_processed_with_anomalies.parquet_dataset ---\n",
      "  Final consolidated hourly dataset saved successfully: /home/ramamurthy/mhnarfth/network_analysis/outputs/ch4/all_routers_hourly_processed_with_anomalies.parquet_dataset\n",
      "\n",
      "--- Verifying final combined hourly dataset (first 5 rows) ---\n",
      "                       in_packets  if_score  if_flag      router\n",
      "t_first                                                         \n",
      "2021-09-30 15:00:00  140513995000  0.156426        1  louisville\n",
      "2021-09-30 16:00:00             0 -0.249702        0  louisville\n",
      "2021-09-30 17:00:00             0 -0.249702        0  louisville\n",
      "2021-09-30 18:00:00             0 -0.249702        0  louisville\n",
      "2021-09-30 19:00:00             0 -0.249702        0  louisville\n",
      "Total rows in final combined hourly dataset: 12604\n",
      "Unique routers in final combined hourly dataset: <StringArray>\n",
      "[  'louisville',       'dallas', 'jacksonville',      'jackson',\n",
      "         'reno',       'boston',   'batonrouge',      'phoenix',\n",
      "       'elpaso',      'atlanta']\n",
      "Length: 10, dtype: string\n",
      "\n",
      "--- Notebook 1 Complete: Data Ingestion, Preprocessing, and EDA ---\n",
      "Check the '/home/ramamurthy/mhnarfth/network_analysis/outputs/ch4' directory for processed hourly Parquet files and figures.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notebook 1: Raw Data Ingestion, Preprocessing, and EDA (Multi-Router)\n",
    "\n",
    "Purpose:\n",
    "  - Load combined raw router data from the multi-part Parquet dataset (using Dask).\n",
    "  - Iterate through each router, filter its data, and process it into a clean, hourly time series,\n",
    "    performing hourly aggregation and initial cleaning within Dask.\n",
    "  - Apply Isolation Forest anomaly detection per router.\n",
    "  - Save processed hourly data with anomaly info for each router.\n",
    "  - Generate global (cross-router) and illustrative (single-router) EDA plots.\n",
    "  - Consolidate all per-router hourly processed files into one final combined hourly Parquet dataset.\n",
    "\n",
    "Outputs:\n",
    "  - outputs/ch4/hourly_<router>_processed_with_anomalies.parquet (10 individual files)\n",
    "  - outputs/ch4/all_routers_hourly_processed_with_anomalies.parquet_dataset (final consolidated hourly data)\n",
    "  - Figures 4-2a, 4-2b, 4-2c, 4-3a, 4-3b, 4-3c saved to outputs/ch4/figs\n",
    "  - Printed Table 4-1 data (Router Inventory).\n",
    "\"\"\"\n",
    "\n",
    "#######################################################################\n",
    "# 0. Environment set‑up                                               #\n",
    "#######################################################################\n",
    "# ☑️  Install all dependencies.\n",
    "# !pip install --quiet pandas pyarrow dask[dataframe] matplotlib seaborn scikit-learn tqdm\n",
    "\n",
    "import os\n",
    "import glob # For listing files, but Dask glob will handle it\n",
    "import warnings\n",
    "import random\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "\n",
    "# For Dask DataFrames (for memory-efficient loading of combined raw data)\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# For Anomaly Detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Suppress minor warnings for cleaner output in Jupyter\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure plots appear inline in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 150 # Increase resolution for better quality plots\n",
    "plt.rcParams['savefig.dpi'] = 300 # Save plots with higher resolution\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); \n",
    "\n",
    "print(\"Environment setup complete.\")\n",
    "\n",
    "#######################################################################\n",
    "# 1. Configuration                                                    #\n",
    "#######################################################################\n",
    "\n",
    "# --- Paths & File Names ---\n",
    "# Input: Path to your combined multi-part Parquet dataset (the directory)\n",
    "# This is the output from the previous script that combined all raw router files.\n",
    "INPUT_COMBINED_RAW_PATH = Path(\"/mnt/nrdstor/ramamurthy/mhnarfth/internet2/parquet/combined/all_routers_combined.parquet_dataset\")\n",
    "\n",
    "# Output root directory for processed data and figures\n",
    "OUT_DIR           = Path(\"outputs/ch4\").absolute(); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR           = OUT_DIR / \"figs\"; FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "# Directory to save individual processed hourly files (intermediate output)\n",
    "PROCESSED_HOURLY_INDIVIDUAL_DIR = OUT_DIR / \"processed_hourly_individual\"; PROCESSED_HOURLY_INDIVIDUAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# Final consolidated hourly processed data (main output of this notebook)\n",
    "FINAL_COMBINED_HOURLY_PATH = OUT_DIR / \"all_routers_hourly_processed_with_anomalies.parquet_dataset\"\n",
    "\n",
    "# --- Data Specifics ---\n",
    "TARGET_COL        = \"in_packets\"           # The volume metric (packets)\n",
    "TIMESTAMP_COL     = \"t_first\"              # Earliest packet timestamp for flow\n",
    "ROUTER_COL        = 'router'\n",
    "# Load only relevant columns from raw data for memory efficiency in Dask\n",
    "COLS_TO_LOAD_RAW = [TIMESTAMP_COL, TARGET_COL, 't_last', ROUTER_COL] # Keep t_last for initial cleaning\n",
    "\n",
    "\n",
    "FREQ              = \"h\"                    # Hourly resampling frequency ('h' for hourly)\n",
    "\n",
    "# --- Anomaly Detection Parameters ---\n",
    "IF_CONTAMINATION  = 0.01 # Initial contamination estimate\n",
    "ANOM_SCORE_COL    = \"if_score\"             # Column name for Isolation Forest anomaly scores\n",
    "ANOM_FLAG_COL     = \"if_flag\"              # Column name for binary anomaly flags (1 = anomaly, 0 = normal)\n",
    "\n",
    "# --- Time Series Parameters (for data splitting, but defined globally) ---\n",
    "TEST_SIZE_HOURS   = 7 * 24                 # 7 days for testing\n",
    "VAL_SIZE_HOURS    = 7 * 24                 # 7 days for validation\n",
    "\n",
    "# --- Global EDA Parameters ---\n",
    "# Fraction of total raw rows for the global flow-size histogram sample.\n",
    "GLOBAL_HISTOGRAM_SAMPLE_FRAC = 0.0005 # A very small fraction (e.g., 0.05% of total raw flows)\n",
    "\n",
    "# Router to generate detailed single-router EDA plots (Fig 4-2b, 4-3a/b/c) for\n",
    "ILLUSTRATIVE_ROUTER = \"elpaso\" # Change to \"atlanta\" or \"dallas\" if preferred\n",
    "\n",
    "\n",
    "print(f\"Configuration loaded.\")\n",
    "print(f\"Input combined raw data: {INPUT_COMBINED_RAW_PATH}\")\n",
    "print(f\"Outputs will be saved to: {OUT_DIR} (figs: {FIG_DIR})\")\n",
    "print(f\"Processed individual hourly files to: {PROCESSED_HOURLY_INDIVIDUAL_DIR}\")\n",
    "print(f\"Final combined hourly dataset to: {FINAL_COMBINED_HOURLY_PATH}\")\n",
    "print(f\"Anomaly Detection contamination (initial): {IF_CONTAMINATION}\")\n",
    "\n",
    "#######################################################################\n",
    "# Helper Functions                                                    #\n",
    "#######################################################################\n",
    "\n",
    "def save_plot(fig_name: str, router_label: str = \"\"): \n",
    "    \"\"\"Saves the current matplotlib figure in both PNG and PDF formats.\"\"\"\n",
    "    plt.tight_layout()\n",
    "    if router_label:\n",
    "        png_path = FIG_DIR / f\"{fig_name}_{router_label}.png\"\n",
    "        pdf_path = FIG_DIR / f\"{fig_name}_{router_label}.pdf\"\n",
    "    else: # For global plots, no router_label in filename\n",
    "        png_path = FIG_DIR / f\"{fig_name}.png\"\n",
    "        pdf_path = FIG_DIR / f\"{fig_name}.pdf\"\n",
    "\n",
    "    plt.savefig(png_path)\n",
    "    plt.savefig(pdf_path)\n",
    "    plt.close()\n",
    "    print(f\"  Saved {png_path.name} and {pdf_path.name}\")\n",
    "\n",
    "\n",
    "def plot_eda_single_router(df_hourly: pd.DataFrame, router_label: str, target_col: str):\n",
    "    \"\"\"\n",
    "    Generates single-router EDA plots (Fig 4-2b, 4-3a/b/c).\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Generating Single-Router EDA Plots for {router_label} ---\")\n",
    "\n",
    "    df_hourly_copy = df_hourly.copy() # Work on a copy to avoid SettingWithCopyWarning\n",
    "    df_hourly_copy['hour_of_day'] = df_hourly_copy.index.hour\n",
    "    df_hourly_copy['day_of_week_num'] = df_hourly_copy.index.dayofweek\n",
    "    df_hourly_copy['day_name'] = df_hourly_copy.index.day_name()\n",
    "    df_hourly_copy['is_weekend'] = df_hourly_copy['day_of_week_num'].isin([5, 6])\n",
    "    \n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "    # Fig 4-2b: Week-of-day heat-map (hour × weekday) - Illustrative for one router\n",
    "    print(f\"  Generating Figure 4-2b (Week-of-Day Heatmap for {router_label})...\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pivot_table = df_hourly_copy.pivot_table(index='hour_of_day', columns='day_name', values=target_col, aggfunc='mean')\n",
    "    pivot_table = pivot_table[day_order]\n",
    "    sns.heatmap(pivot_table, cmap='viridis', annot=False, fmt=\".0f\", linewidths=.5, linecolor='lightgray')\n",
    "    plt.title(f\"4-2b Average {target_col} Heatmap: Hour of Day vs. Day of Week ({router_label})\")\n",
    "    plt.xlabel(\"Day of Week\")\n",
    "    plt.ylabel(\"Hour of Day\")\n",
    "    save_plot(f\"4_2b_weekday_heatmap\", router_label)\n",
    "\n",
    "    # Anomaly Detection Plots (Figures 4-3a/b/c)\n",
    "    if ANOM_SCORE_COL in df_hourly_copy.columns and ANOM_FLAG_COL in df_hourly_copy.columns:\n",
    "        print(f\"\\n--- Generating Anomaly Detection Plots for {router_label} ---\")\n",
    "        # Fig 4-3a: Isolation-Forest score histogram\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sorted_scores = np.sort(df_hourly_copy[ANOM_SCORE_COL].values)\n",
    "        threshold_idx = int((1 - IF_CONTAMINATION) * len(sorted_scores))\n",
    "        threshold_score = sorted_scores[threshold_idx]\n",
    "        sns.histplot(df_hourly_copy[ANOM_SCORE_COL], bins=100, kde=True, color='teal', alpha=0.7)\n",
    "        plt.axvline(x=threshold_score, color='red', linestyle='--', label=f'Threshold (Contamination={IF_CONTAMINATION})')\n",
    "        plt.title(f\"4-3a Isolation Forest Anomaly Score Histogram ({router_label})\")\n",
    "        plt.xlabel(\"Anomaly Score (Higher = More Anomalous)\")\n",
    "        plt.ylabel(\"Number of Observations\")\n",
    "        plt.legend()\n",
    "        save_plot(f\"4_3a_if_score_histogram\", router_label)\n",
    "\n",
    "        # Fig 4-3b: Time-series plot with anomalies marked\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.plot(df_hourly_copy.index, df_hourly_copy[target_col], label='Original Data', color='blue', alpha=0.7)\n",
    "        anomalies = df_hourly_copy[df_hourly_copy[ANOM_FLAG_COL] == 1]\n",
    "        plt.scatter(anomalies.index, anomalies[target_col], color='red', s=50, label='Detected Anomaly', zorder=5)\n",
    "        plt.title(f\"4-3b {target_col} Time Series with Detected Anomalies ({router_label})\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(f\"{target_col}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle=':', alpha=0.7)\n",
    "        save_plot(f\"4_3b_timeseries_anomalies\", router_label)\n",
    "\n",
    "        # Fig 4-3c: Anomaly count bar-chart by day of week\n",
    "        anomaly_counts_per_day = df_hourly_copy[df_hourly_copy[ANOM_FLAG_COL] == 1]['day_name'].value_counts().reindex(day_order).fillna(0)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=anomaly_counts_per_day.index, y=anomaly_counts_per_day.values, palette='viridis')\n",
    "        plt.title(f\"4-3c Anomaly Counts by Day of Week ({router_label})\")\n",
    "        plt.xlabel(\"Day of Week\")\n",
    "        plt.ylabel(\"Number of Anomalies\")\n",
    "        plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "        save_plot(f\"4_3c_anomaly_counts_by_day\", router_label)\n",
    "    else:\n",
    "        print(f\"  Anomaly columns not found in df_hourly for {router_label}. Skipping anomaly plots.\")\n",
    "\n",
    "def plot_global_eda(all_hourly_mean_per_hour: pd.DataFrame, all_raw_samples_target_col: pd.Series, target_col: str):\n",
    "    \"\"\"\n",
    "    Generates global (cross-router) EDA plots (Fig 4-2a, 4-2c).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Global EDA Plots (Figures 4-2a, 4-2c) ---\")\n",
    "\n",
    "    # Fig 4-2a: 10-router daily curves (mean traffic per hour)\n",
    "    if not all_hourly_mean_per_hour.empty: # Only plot if data is provided\n",
    "        print(\"  Generating Figure 4-2a (10-Router Daily Curves)...\")\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        sns.lineplot(x='hour_of_day', y='mean_packets', hue='router', data=all_hourly_mean_per_hour, palette='tab10')\n",
    "        plt.title(f\"4-2a Daily Utilization Curves (Mean {target_col} per Hour Across Routers)\")\n",
    "        plt.xlabel(\"Hour of Day (0-23)\")\n",
    "        plt.ylabel(f\"Mean {target_col}\")\n",
    "        plt.xticks(range(24))\n",
    "        plt.legend(title='Router', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, linestyle=':', alpha=0.7)\n",
    "        save_plot(f\"4_2a_daily_curves_all_routers\", router_label=\"\")\n",
    "    else:\n",
    "        print(\"  No hourly mean data collected. Skipping Fig 4-2a.\")\n",
    "\n",
    "    # Fig 4-2c: Flow-size distribution (log-histogram of raw packets)\n",
    "    if not all_raw_samples_target_col.empty: # Only plot if data is provided\n",
    "        print(\"  Generating Figure 4-2c (Flow Size Distribution - All Routers Sample)...\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(np.log1p(all_raw_samples_target_col), bins=50, kde=True, color='purple', alpha=0.7)\n",
    "        plt.title(f\"4-2c Log-Histogram of Flow Sizes (All Routers Sample)\")\n",
    "        plt.xlabel(f\"log(1 + {target_col})\")\n",
    "        plt.ylabel(\"Number of Flows (Count)\")\n",
    "        plt.grid(True, linestyle=':', alpha=0.7)\n",
    "        save_plot(f\"4_2c_flow_size_histogram_all_routers\", router_label=\"\")\n",
    "    else:\n",
    "        print(f\"  No raw samples data for flow-size histogram. Skipping Fig 4-2c.\")\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# Anomaly Detection (Isolation Forest)                                #\n",
    "#######################################################################\n",
    "\n",
    "def run_isolation_forest(series: pd.Series, contamination: float, router_label: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fits Isolation Forest on the time series and returns anomaly scores and flags.\n",
    "    The input `series` should be the hourly aggregated data (e.g., in_packets).\n",
    "    \"\"\"\n",
    "    print(f\"  Running Isolation Forest for Anomaly Detection for {router_label}...\")\n",
    "    \n",
    "    if series.empty:\n",
    "        print(f\"  Warning: Input series for Isolation Forest is empty for {router_label}. Skipping.\")\n",
    "        return pd.DataFrame(columns=[series.name, ANOM_SCORE_COL, ANOM_FLAG_COL])\n",
    "\n",
    "    X = series.values.reshape(-1, 1)\n",
    "    \n",
    "    iforest = IsolationForest(n_estimators=200, contamination=contamination, random_state=SEED, n_jobs=-1) \n",
    "    iforest.fit(X)\n",
    "    \n",
    "    scores = -iforest.decision_function(X)\n",
    "    flags = iforest.predict(X)\n",
    "    flags = np.where(flags == -1, 1, 0)\n",
    "    \n",
    "    out_df = pd.DataFrame({\n",
    "        series.name: series.values,\n",
    "        ANOM_SCORE_COL: scores,\n",
    "        ANOM_FLAG_COL: flags\n",
    "    }, index=series.index)\n",
    "    \n",
    "    print(f\"  Isolation Forest detection complete for {router_label}. Found {out_df[ANOM_FLAG_COL].sum()} anomalies.\")\n",
    "    return out_df\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# Main Execution for Notebook 1                                       #\n",
    "#######################################################################\n",
    "\n",
    "def main():\n",
    "    print(\"\\n--- Initiating Notebook 1: Data Ingestion, Preprocessing, and EDA (Multi-Router) ---\")\n",
    "\n",
    "    # 1. Load the combined raw router data (Dask DataFrame)\n",
    "    print(f\"\\nLoading combined raw data from: {INPUT_COMBINED_RAW_PATH}\")\n",
    "    try:\n",
    "        # Load only necessary columns for the first stage (timestamps, target, router_col)\n",
    "        combined_ddf_raw = dd.read_parquet(INPUT_COMBINED_RAW_PATH, engine=\"pyarrow\", columns=COLS_TO_LOAD_RAW)\n",
    "        combined_ddf_raw[ROUTER_COL] = combined_ddf_raw[ROUTER_COL].astype(str)\n",
    "        print(f\"Loaded Dask DataFrame (lazy): {combined_ddf_raw.npartitions} partitions.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading combined raw Dask DataFrame: {e}. Please ensure the path is correct and files exist.\")\n",
    "        return\n",
    "\n",
    "    # Get unique router IDs (this will trigger a small Dask compute)\n",
    "    unique_routers = combined_ddf_raw[ROUTER_COL].unique().compute().tolist()\n",
    "    unique_routers.sort() \n",
    "    print(f\"Found {len(unique_routers)} unique routers: {unique_routers}\")\n",
    "\n",
    "    # Data structures to collect info for global EDA plots and Table 4-1\n",
    "    router_inventory_data = [] \n",
    "    all_hourly_mean_per_hour_list = [] \n",
    "    \n",
    "    # NEW FIX for Fig 4-2c (Flow-size Distribution): Collect GLOBAL_HISTOGRAM_SAMPLE_FRAC of total raw data once at the beginning.\n",
    "    print(f\"\\nCollecting {GLOBAL_HISTOGRAM_SAMPLE_FRAC*100:.2f}% of total raw data for Fig 4-2c (Flow-size Distribution)...\")\n",
    "    try:\n",
    "        # Select only the target column from the Dask DataFrame and compute the sample\n",
    "        all_raw_samples_target_col_series = combined_ddf_raw[TARGET_COL].sample(frac=GLOBAL_HISTOGRAM_SAMPLE_FRAC, random_state=SEED).compute()\n",
    "        print(f\"  Collected {len(all_raw_samples_target_col_series)} raw flow samples for global histogram.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error sampling raw data for flow-size histogram: {e}. Skipping Fig 4-2c. Error: {e}\")\n",
    "        all_raw_samples_target_col_series = pd.Series(dtype=float) # Create empty Series to avoid further errors\n",
    "\n",
    "\n",
    "    # Process each router sequentially\n",
    "    # The progress bar is around this loop\n",
    "    for router_idx, router_label in enumerate(tqdm(unique_routers, desc=\"Processing Routers\")):\n",
    "        print(f\"\\n==== Starting Full Processing for Router: {router_label} ({router_idx + 1}/{len(unique_routers)}) ====\")\n",
    "        \n",
    "        # Step 1: Filter raw data for current router (still Dask)\n",
    "        ddf_single_router = combined_ddf_raw[combined_ddf_raw[ROUTER_COL] == router_label]\n",
    "        \n",
    "        # Collect raw data stats for Table 4-1\n",
    "        num_flows_raw = ddf_single_router.shape[0].compute() \n",
    "        print(f\"  Raw data for {router_label}: {num_flows_raw} records.\")\n",
    "\n",
    "\n",
    "        # Step 2: Dask-native Initial Clean and Hourly Resampling\n",
    "        print(f\"  Performing Dask-native initial cleaning and hourly resampling for {router_label}...\")\n",
    "        \n",
    "        # Define the Dask computation graph for a single router's hourly aggregation\n",
    "        # This graph will be executed when `.compute()` is called.\n",
    "        ddf_hourly_router_pipeline = (\n",
    "            ddf_single_router[[TIMESTAMP_COL, TARGET_COL, 't_last']] # Include t_last for accurate initial cleaning within Dask\n",
    "            .assign(**{TIMESTAMP_COL: lambda df: dd.to_datetime(df[TIMESTAMP_COL], errors='coerce')})\n",
    "            # NEW FIX: Also convert 't_last' to datetime if you intend to drop rows based on it\n",
    "            .assign(t_last_dt=lambda df: dd.to_datetime(df['t_last'], errors='coerce'))\n",
    "            .dropna(subset=[TIMESTAMP_COL]) # Drop rows where t_first is NaT\n",
    "            .set_index(TIMESTAMP_COL, sorted=True) # Set index for resampling\n",
    "            .resample(FREQ) # Resample to hourly\n",
    "            .agg({TARGET_COL: 'sum'}) # Sum target column\n",
    "        )\n",
    "        \n",
    "        # Add router column (still Dask)\n",
    "        ddf_hourly_router_pipeline[ROUTER_COL] = router_label\n",
    "\n",
    "        # NOW, compute this hourly aggregated Dask DataFrame into Pandas.\n",
    "        # This is the crucial step: df_hourly_resampled should be small (max ~1368 rows).\n",
    "        df_hourly_resampled = ddf_hourly_router_pipeline.compute() \n",
    "        print(f\"  Computed hourly data for {router_label}: {len(df_hourly_resampled)} rows.\")\n",
    "        # Explicitly delete Dask DataFrames to free up graph memory\n",
    "        del ddf_single_router, ddf_hourly_router_pipeline \n",
    "        gc.collect()\n",
    "\n",
    "        # Pandas Post-Resampling (Interpolation/Fill)\n",
    "        if df_hourly_resampled.empty:\n",
    "            print(f\"  Warning: Hourly resampled data for {router_label} is empty. Skipping further processing for this router.\")\n",
    "            num_hourly_rows = 0\n",
    "            router_inventory_data.append({\n",
    "                \"Router ID\": router_label,\n",
    "                \"Raw Flows (#)\": num_flows_raw,\n",
    "                \"Hourly Rows (#)\": num_hourly_rows\n",
    "            })\n",
    "            continue \n",
    "        \n",
    "        missing_before_interp = df_hourly_resampled[TARGET_COL].isnull().sum()\n",
    "        df_hourly_resampled[TARGET_COL] = df_hourly_resampled[TARGET_COL].interpolate(method='linear')\n",
    "        df_hourly_resampled[TARGET_COL].fillna(0, inplace=True)\n",
    "        if missing_before_interp > 0:\n",
    "            print(f\"  Interpolated {missing_before_interp} missing values linearly for hourly data.\")\n",
    "        \n",
    "\n",
    "        # Collect hourly data stats for Table 4-1\n",
    "        num_hourly_rows = len(df_hourly_resampled)\n",
    "        router_inventory_data.append({\n",
    "            \"Router ID\": router_label,\n",
    "            \"Raw Flows (#)\": num_flows_raw,\n",
    "            \"Hourly Rows (#)\": num_hourly_rows\n",
    "        })\n",
    "\n",
    "\n",
    "        # 5. Anomaly Detection (Isolation Forest) (on Pandas df_hourly_resampled)\n",
    "        if df_hourly_resampled.empty or TARGET_COL not in df_hourly_resampled.columns:\n",
    "            print(f\"  Warning: Resampled data for {router_label} is empty or missing target column. Skipping anomaly detection.\")\n",
    "            df_hourly_with_anomalies = df_hourly_resampled.copy()\n",
    "            df_hourly_with_anomalies[ANOM_SCORE_COL] = np.nan\n",
    "            df_hourly_with_anomalies[ANOM_FLAG_COL] = 0\n",
    "        else:\n",
    "            df_hourly_with_anomalies = run_isolation_forest(\n",
    "                df_hourly_resampled[TARGET_COL], IF_CONTAMINATION, router_label\n",
    "            )\n",
    "            df_hourly_with_anomalies[ROUTER_COL] = router_label\n",
    "        del df_hourly_resampled \n",
    "        gc.collect()\n",
    "\n",
    "        # Save the processed hourly data for this router for Notebook 2/3\n",
    "        processed_hourly_parquet_path = PROCESSED_HOURLY_INDIVIDUAL_DIR / f\"hourly_{router_label.lower().replace(' ', '_')}_processed_with_anomalies.parquet\"\n",
    "        print(f\"  Saving processed hourly data for {router_label} to: {processed_hourly_parquet_path}\")\n",
    "        df_hourly_with_anomalies.to_parquet(processed_hourly_parquet_path, index=True) \n",
    "        print(f\"  Processed hourly data for {router_label} saved successfully.\")\n",
    "\n",
    "        # 4. Illustrative EDA Plots (Single Router: 4-2b, 4-3a/b/c)\n",
    "        if router_label == ILLUSTRATIVE_ROUTER: \n",
    "            plot_eda_single_router(df_hourly_with_anomalies.copy(), router_label, TARGET_COL)\n",
    "        else:\n",
    "            print(f\"  Skipping single-router EDA plots for {router_label} (generating for {ILLUSTRATIVE_ROUTER} as representative).\")\n",
    "\n",
    "        del df_hourly_with_anomalies \n",
    "        gc.collect()\n",
    "\n",
    "    # After processing all routers, plot global EDA\n",
    "    print(\"\\n==== All Routers Processed. Generating Global EDA Plots ====\")\n",
    "    \n",
    "    # Fig 4-2a: 10-router daily curves\n",
    "    if all_hourly_mean_per_hour_list:\n",
    "        combined_hourly_mean_per_hour_df = pd.concat(all_hourly_mean_per_hour_list, ignore_index=True)\n",
    "        # Pass the correctly collected df for plotting\n",
    "        plot_global_eda(combined_hourly_mean_per_hour_df, pd.Series(dtype=float), TARGET_COL) \n",
    "    else:\n",
    "        print(\"  No hourly mean data collected. Skipping Fig 4-2a.\")\n",
    "    \n",
    "    # Fig 4-2c: Flow-size distribution (uses all_raw_samples_target_col_series computed earlier)\n",
    "    if not all_raw_samples_target_col_series.empty:\n",
    "        # Pass the correctly collected df for plotting\n",
    "        plot_global_eda(pd.DataFrame(), all_raw_samples_target_col_series, TARGET_COL) \n",
    "    else:\n",
    "        print(\"  No raw samples collected. Skipping Fig 4-2c.\")\n",
    "\n",
    "    # Print Table 4-1: Router Inventory\n",
    "    print(\"\\n--- Table 4-1: Router Inventory (Raw Data & Hourly Aggregates) ---\")\n",
    "    router_inventory_df = pd.DataFrame(router_inventory_data)\n",
    "    inventory_cols = [\"Router ID\", \"Raw Flows (#)\", \"Hourly Rows (#)\"]\n",
    "    router_inventory_df = router_inventory_df[inventory_cols] \n",
    "    print(router_inventory_df.to_string())\n",
    "\n",
    "    # Final Consolidation of all hourly processed files\n",
    "    print(f\"\\n--- Consolidating all individual hourly processed files to: {FINAL_COMBINED_HOURLY_PATH} ---\")\n",
    "    try:\n",
    "        # Read all individual hourly processed Parquet files using Dask\n",
    "        hourly_processed_files = list(PROCESSED_HOURLY_INDIVIDUAL_DIR.glob(\"*.parquet\"))\n",
    "        if not hourly_processed_files:\n",
    "            print(\"  No individual hourly processed files found for final consolidation. Skipping.\")\n",
    "            return\n",
    "\n",
    "        ddf_list_hourly = [dd.read_parquet(str(f)) for f in hourly_processed_files] # Use str(f) for Dask compatibility\n",
    "        combined_ddf_hourly = dd.concat(ddf_list_hourly, axis=0, ignore_index=False) # Keep original index (timestamps)\n",
    "        \n",
    "        # Write to final combined hourly dataset\n",
    "        combined_ddf_hourly.to_parquet(FINAL_COMBINED_HOURLY_PATH, engine=\"pyarrow\", write_index=True)\n",
    "        print(f\"  Final consolidated hourly dataset saved successfully: {FINAL_COMBINED_HOURLY_PATH}\")\n",
    "\n",
    "        # Optional: Verify final combined hourly file\n",
    "        print(f\"\\n--- Verifying final combined hourly dataset (first 5 rows) ---\")\n",
    "        verified_df_hourly = pd.read_parquet(FINAL_COMBINED_HOURLY_PATH)\n",
    "        print(verified_df_hourly.head())\n",
    "        print(f\"Total rows in final combined hourly dataset: {len(verified_df_hourly)}\")\n",
    "        print(f\"Unique routers in final combined hourly dataset: {verified_df_hourly[ROUTER_COL].unique()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during final hourly consolidation: {e}\")\n",
    "        print(\"Please check intermediary files and disk space.\")\n",
    "\n",
    "    print(\"\\n--- Notebook 1 Complete: Data Ingestion, Preprocessing, and EDA ---\")\n",
    "    print(f\"Check the '{OUT_DIR}' directory for processed hourly Parquet files and figures.\")\n",
    "\n",
    "# Execute the main pipeline for this notebook\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mahin's Thesis Venv",
   "language": "python",
   "name": "mahin_mthesis_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
