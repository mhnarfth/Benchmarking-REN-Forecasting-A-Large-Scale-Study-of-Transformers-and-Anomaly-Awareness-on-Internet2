{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75040377-7c89-487d-92c5-5335e9fe3d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramamurthy/mhnarfth/network_analysis/mahin_mThesis_venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete.\n",
      "Configuration loaded.\n",
      "Input individual router Parquet files from: /mnt/nrdstor/ramamurthy/mhnarfth/internet2/parquet\n",
      "Outputs will be saved to: /home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4 (figs: /home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4/figs)\n",
      "Anomaly Detection contamination (initial): 0.01\n",
      "\n",
      "--- Initiating Notebook 1: Raw Data ETL & Anomaly Flagging (Multi-Router) ---\n",
      "\n",
      "Found 10 individual router Parquet files in /mnt/nrdstor/ramamurthy/mhnarfth/internet2/parquet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Starting Full Processing for Router: atlanta (1/10) ====\n",
      "  Loading raw data for atlanta from atlanta.parquet with selected columns...\n",
      "  Loaded Dask DataFrame (lazy) for atlanta. Partitions: 8\n",
      "  Raw data for atlanta: 28032240 records.\n",
      "  Collected 10000 raw samples for global histogram from atlanta.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for atlanta...\n",
      "  Computed hourly data for atlanta: 1369 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for atlanta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  10%|█         | 1/10 [00:29<04:22, 29.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for atlanta. Found 13 anomalies.\n",
      "  Saving processed hourly data for atlanta to: /home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4/hourly_atlanta_processed_with_anomalies.parquet\n",
      "  Processed hourly data for atlanta saved successfully.\n",
      "  Skipping single-router EDA plots for atlanta (generating for El Paso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: batonrouge (2/10) ====\n",
      "  Loading raw data for batonrouge from batonrouge.parquet with selected columns...\n",
      "  Loaded Dask DataFrame (lazy) for batonrouge. Partitions: 9\n",
      "  Raw data for batonrouge: 80056602 records.\n",
      "  Collected 10001 raw samples for global histogram from batonrouge.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for batonrouge...\n",
      "  Computed hourly data for batonrouge: 1343 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for batonrouge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  20%|██        | 2/10 [01:41<07:18, 54.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for batonrouge. Found 14 anomalies.\n",
      "  Saving processed hourly data for batonrouge to: /home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4/hourly_batonrouge_processed_with_anomalies.parquet\n",
      "  Processed hourly data for batonrouge saved successfully.\n",
      "  Skipping single-router EDA plots for batonrouge (generating for El Paso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: boston (3/10) ====\n",
      "  Loading raw data for boston from boston.parquet with selected columns...\n",
      "  Loaded Dask DataFrame (lazy) for boston. Partitions: 1\n",
      "  Raw data for boston: 392891 records.\n",
      "  Collected 10000 raw samples for global histogram from boston.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for boston...\n",
      "  Computed hourly data for boston: 789 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for boston...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  30%|███       | 3/10 [01:42<03:31, 30.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for boston. Found 8 anomalies.\n",
      "  Saving processed hourly data for boston to: /home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4/hourly_boston_processed_with_anomalies.parquet\n",
      "  Processed hourly data for boston saved successfully.\n",
      "  Skipping single-router EDA plots for boston (generating for El Paso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: dallas (4/10) ====\n",
      "  Loading raw data for dallas from dallas.parquet with selected columns...\n",
      "  Loaded Dask DataFrame (lazy) for dallas. Partitions: 17\n",
      "  Raw data for dallas: 149784626 records.\n",
      "  Collected 10001 raw samples for global histogram from dallas.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for dallas...\n",
      "  Computed hourly data for dallas: 1322 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for dallas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  40%|████      | 4/10 [03:53<06:58, 69.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for dallas. Found 14 anomalies.\n",
      "  Saving processed hourly data for dallas to: /home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4/hourly_dallas_processed_with_anomalies.parquet\n",
      "  Processed hourly data for dallas saved successfully.\n",
      "  Skipping single-router EDA plots for dallas (generating for El Paso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: elpaso (5/10) ====\n",
      "  Loading raw data for elpaso from elpaso.parquet with selected columns...\n",
      "  Loaded Dask DataFrame (lazy) for elpaso. Partitions: 3\n",
      "  Raw data for elpaso: 9357137 records.\n",
      "  Collected 10000 raw samples for global histogram from elpaso.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for elpaso...\n",
      "  Computed hourly data for elpaso: 1378 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for elpaso...\n",
      "  Isolation Forest detection complete for elpaso. Found 14 anomalies.\n",
      "  Saving processed hourly data for elpaso to: /home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4/hourly_elpaso_processed_with_anomalies.parquet\n",
      "  Processed hourly data for elpaso saved successfully.\n",
      "\n",
      "--- Generating Single-Router EDA Plots for elpaso ---\n",
      "  Generating Figure 4-2b (Week-of-Day Heatmap for elpaso)...\n",
      "  Saved 4_2b_weekday_heatmap_elpaso.png and 4_2b_weekday_heatmap_elpaso.pdf\n",
      "\n",
      "--- Generating Anomaly Detection Plots for elpaso ---\n",
      "  Saved 4_3a_if_score_histogram_elpaso.png and 4_3a_if_score_histogram_elpaso.pdf\n",
      "  Saved 4_3b_timeseries_anomalies_elpaso.png and 4_3b_timeseries_anomalies_elpaso.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  50%|█████     | 5/10 [04:06<04:06, 49.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 4_3c_anomaly_counts_by_day_elpaso.png and 4_3c_anomaly_counts_by_day_elpaso.pdf\n",
      "\n",
      "==== Starting Full Processing for Router: jackson (6/10) ====\n",
      "  Loading raw data for jackson from jackson.parquet with selected columns...\n",
      "  Loaded Dask DataFrame (lazy) for jackson. Partitions: 4\n",
      "  Raw data for jackson: 13401496 records.\n",
      "  Collected 10000 raw samples for global histogram from jackson.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for jackson...\n",
      "  Computed hourly data for jackson: 1369 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for jackson...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  60%|██████    | 6/10 [04:21<02:30, 37.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for jackson. Found 14 anomalies.\n",
      "  Saving processed hourly data for jackson to: /home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4/hourly_jackson_processed_with_anomalies.parquet\n",
      "  Processed hourly data for jackson saved successfully.\n",
      "  Skipping single-router EDA plots for jackson (generating for El Paso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: jacksonville (7/10) ====\n",
      "  Loading raw data for jacksonville from jacksonville.parquet with selected columns...\n",
      "  Loaded Dask DataFrame (lazy) for jacksonville. Partitions: 5\n",
      "  Raw data for jacksonville: 19425447 records.\n",
      "  Collected 10000 raw samples for global histogram from jacksonville.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for jacksonville...\n",
      "  Computed hourly data for jacksonville: 1338 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for jacksonville...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  70%|███████   | 7/10 [04:42<01:37, 32.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for jacksonville. Found 14 anomalies.\n",
      "  Saving processed hourly data for jacksonville to: /home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4/hourly_jacksonville_processed_with_anomalies.parquet\n",
      "  Processed hourly data for jacksonville saved successfully.\n",
      "  Skipping single-router EDA plots for jacksonville (generating for El Paso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: louisville (8/10) ====\n",
      "  Loading raw data for louisville from louisville.parquet with selected columns...\n",
      "  Loaded Dask DataFrame (lazy) for louisville. Partitions: 1\n",
      "  Raw data for louisville: 3150312 records.\n",
      "  Collected 10000 raw samples for global histogram from louisville.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for louisville...\n",
      "  Computed hourly data for louisville: 1466 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for louisville...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  80%|████████  | 8/10 [04:47<00:47, 23.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for louisville. Found 15 anomalies.\n",
      "  Saving processed hourly data for louisville to: /home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4/hourly_louisville_processed_with_anomalies.parquet\n",
      "  Processed hourly data for louisville saved successfully.\n",
      "  Skipping single-router EDA plots for louisville (generating for El Paso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: phoenix (9/10) ====\n",
      "  Loading raw data for phoenix from phoenix.parquet with selected columns...\n",
      "  Loaded Dask DataFrame (lazy) for phoenix. Partitions: 3\n",
      "  Raw data for phoenix: 8765751 records.\n",
      "  Collected 10000 raw samples for global histogram from phoenix.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for phoenix...\n",
      "  Computed hourly data for phoenix: 1369 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for phoenix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  90%|█████████ | 9/10 [04:57<00:19, 19.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for phoenix. Found 14 anomalies.\n",
      "  Saving processed hourly data for phoenix to: /home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4/hourly_phoenix_processed_with_anomalies.parquet\n",
      "  Processed hourly data for phoenix saved successfully.\n",
      "  Skipping single-router EDA plots for phoenix (generating for El Paso as representative).\n",
      "\n",
      "==== Starting Full Processing for Router: reno (10/10) ====\n",
      "  Loading raw data for reno from reno.parquet with selected columns...\n",
      "  Loaded Dask DataFrame (lazy) for reno. Partitions: 4\n",
      "  Raw data for reno: 35499401 records.\n",
      "  Collected 10000 raw samples for global histogram from reno.\n",
      "  Performing Dask-native initial cleaning and hourly resampling for reno...\n",
      "  Computed hourly data for reno: 861 rows.\n",
      "  Running Isolation Forest for Anomaly Detection for reno...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers: 100%|██████████| 10/10 [05:32<00:00, 33.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Isolation Forest detection complete for reno. Found 9 anomalies.\n",
      "  Saving processed hourly data for reno to: /home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4/hourly_reno_processed_with_anomalies.parquet\n",
      "  Processed hourly data for reno saved successfully.\n",
      "  Skipping single-router EDA plots for reno (generating for El Paso as representative).\n",
      "\n",
      "==== All Routers Processed. Generating Global EDA Plots ====\n",
      "\n",
      "--- Generating Global EDA Plots (Figures 4-2a, 4-2c) ---\n",
      "  Generating Figure 4-2a (10-Router Daily Curves)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 4_2a_daily_curves_all_routers.png and 4_2a_daily_curves_all_routers.pdf\n",
      "  No raw samples data for flow-size histogram. Skipping Fig 4-2c.\n",
      "\n",
      "--- Generating Global EDA Plots (Figures 4-2a, 4-2c) ---\n",
      "  No hourly mean data collected. Skipping Fig 4-2a.\n",
      "  Generating Figure 4-2c (Flow Size Distribution - All Routers Sample)...\n",
      "  Saved 4_2c_flow_size_histogram_all_routers.png and 4_2c_flow_size_histogram_all_routers.pdf\n",
      "\n",
      "--- Table 4-1: Router Inventory (Raw Data & Hourly Aggregates) ---\n",
      "      Router ID  Raw Flows (#)  Hourly Rows (#)\n",
      "0       atlanta       28032240             1369\n",
      "1    batonrouge       80056602             1343\n",
      "2        boston         392891              789\n",
      "3        dallas      149784626             1322\n",
      "4        elpaso        9357137             1378\n",
      "5       jackson       13401496             1369\n",
      "6  jacksonville       19425447             1338\n",
      "7    louisville        3150312             1466\n",
      "8       phoenix        8765751             1369\n",
      "9          reno       35499401              861\n",
      "\n",
      "--- Notebook 1 Complete: Raw Data ETL & Anomaly Flagging ---\n",
      "Check the '/home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4' directory for processed hourly Parquet files (e.g., hourly_atlanta_processed_with_anomalies.parquet).\n",
      "Check the '/home/ramamurthy/mhnarfth/network_analysis/individually process kora/outputs/ch4/figs' directory for generated figures.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notebook 1: Raw Data ETL & Anomaly Flagging (Multi-Router)\n",
    "Purpose:\n",
    "  - Iterate through individual raw router Parquet files.\n",
    "  - For each router:\n",
    "    - Load raw data (Dask DataFrame, with column selection).\n",
    "    - Perform Dask-native initial cleaning and hourly aggregation.\n",
    "    - Compute the hourly aggregated data to a Pandas DataFrame (memory-safe).\n",
    "    - Run Isolation Forest anomaly detection.\n",
    "    - Save the processed hourly data with anomaly info to a dedicated Parquet file.\n",
    "  - Generate global (cross-router) and illustrative (single-router) EDA plots.\n",
    "\n",
    "Outputs:\n",
    "  - outputs/ch4/hourly_<router>_processed_with_anomalies.parquet (10 files)\n",
    "  - Figures 4-2a, 4-2b, 4-2c, 4-3a, 4-3b, 4-3c saved to outputs/ch4/figs\n",
    "  - Printed Table 4-1 (Router Inventory).\n",
    "\"\"\"\n",
    "\n",
    "#######################################################################\n",
    "# 0. Environment set‑up                                               #\n",
    "#######################################################################\n",
    "# ☑️  Install all dependencies.\n",
    "# !pip install --quiet pandas pyarrow dask[dataframe] matplotlib seaborn scikit-learn tqdm\n",
    "\n",
    "import os\n",
    "import glob # To find individual Parquet files\n",
    "import warnings\n",
    "import random\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "\n",
    "# For Dask DataFrames\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# For Anomaly Detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Suppress minor warnings for cleaner output in Jupyter\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure plots appear inline in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 150 # Increase resolution for better quality plots\n",
    "plt.rcParams['savefig.dpi'] = 300 # Save plots with higher resolution\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); \n",
    "\n",
    "print(\"Environment setup complete.\")\n",
    "\n",
    "#######################################################################\n",
    "# 1. Configuration                                                    #\n",
    "#######################################################################\n",
    "\n",
    "# --- Paths & File Names ---\n",
    "# Input: Directory containing your INDIVIDUAL router Parquet files\n",
    "INPUT_ROUTERS_DIR = Path(\"/mnt/nrdstor/ramamurthy/mhnarfth/internet2/parquet\")\n",
    "\n",
    "# Output root directory for processed data and figures\n",
    "OUT_DIR           = Path(\"outputs/ch4\").absolute(); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR           = OUT_DIR / \"figs\"; FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# --- Data Specifics ---\n",
    "TARGET_COL        = \"in_packets\"           # The volume metric (packets)\n",
    "TIMESTAMP_COL     = \"t_first\"              # Earliest packet timestamp for flow\n",
    "# 't_last' is also needed for initial loading/cleaning, even if not directly used later.\n",
    "ADDITIONAL_RAW_COLS = ['t_last'] # Columns to load besides TIMESTAMP_COL and TARGET_COL\n",
    "FREQ              = \"h\"                    # Hourly resampling frequency ('h' for hourly)\n",
    "ROUTER_COL        = \"router\"               # Column for router name (will be added to processed files)\n",
    "\n",
    "# --- Anomaly Detection Parameters ---\n",
    "IF_CONTAMINATION  = 0.01 # Initial contamination estimate\n",
    "ANOM_SCORE_COL    = \"if_score\"             # Column name for Isolation Forest anomaly scores\n",
    "ANOM_FLAG_COL     = \"if_flag\"              # Column name for binary anomaly flags (1 = anomaly, 0 = normal)\n",
    "\n",
    "# --- Global EDA Parameters ---\n",
    "# Fraction of total rows for the global flow-size histogram sample.\n",
    "# This will be sampled from EACH router's raw data for representation.\n",
    "GLOBAL_HISTOGRAM_SAMPLE_PER_ROUTER_COUNT = 10_000 # Sample up to 10,000 raw rows from each router\n",
    "\n",
    "print(f\"Configuration loaded.\")\n",
    "print(f\"Input individual router Parquet files from: {INPUT_ROUTERS_DIR}\")\n",
    "print(f\"Outputs will be saved to: {OUT_DIR} (figs: {FIG_DIR})\")\n",
    "print(f\"Anomaly Detection contamination (initial): {IF_CONTAMINATION}\")\n",
    "\n",
    "#######################################################################\n",
    "# Helper Functions                                                    #\n",
    "#######################################################################\n",
    "\n",
    "# Helper function to save plots\n",
    "def save_plot(fig_name: str, router_label: str = \"\"): \n",
    "    \"\"\"Saves the current matplotlib figure in both PNG and PDF formats.\"\"\"\n",
    "    plt.tight_layout()\n",
    "    if router_label:\n",
    "        png_path = FIG_DIR / f\"{fig_name}_{router_label}.png\"\n",
    "        pdf_path = FIG_DIR / f\"{fig_name}_{router_label}.pdf\"\n",
    "    else: # For global plots, no router_label in filename\n",
    "        png_path = FIG_DIR / f\"{fig_name}.png\"\n",
    "        pdf_path = FIG_DIR / f\"{fig_name}.pdf\"\n",
    "\n",
    "    plt.savefig(png_path)\n",
    "    plt.savefig(pdf_path)\n",
    "    plt.close()\n",
    "    print(f\"  Saved {png_path.name} and {pdf_path.name}\")\n",
    "\n",
    "\n",
    "def plot_eda_single_router(df_hourly: pd.DataFrame, router_label: str, target_col: str):\n",
    "    \"\"\"\n",
    "    Generates single-router EDA plots (Fig 4-2b, 4-3a/b/c).\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Generating Single-Router EDA Plots for {router_label} ---\")\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning when adding columns\n",
    "    df_hourly_copy = df_hourly.copy() \n",
    "    df_hourly_copy['hour_of_day'] = df_hourly_copy.index.hour\n",
    "    df_hourly_copy['day_of_week_num'] = df_hourly_copy.index.dayofweek\n",
    "    df_hourly_copy['day_name'] = df_hourly_copy.index.day_name()\n",
    "    df_hourly_copy['is_weekend'] = df_hourly_copy['day_of_week_num'].isin([5, 6])\n",
    "    \n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "    # Fig 4-2b: Week-of-day heat-map (hour × weekday) - Illustrative for one router\n",
    "    print(f\"  Generating Figure 4-2b (Week-of-Day Heatmap for {router_label})...\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pivot_table = df_hourly_copy.pivot_table(index='hour_of_day', columns='day_name', values=target_col, aggfunc='mean')\n",
    "    pivot_table = pivot_table[day_order]\n",
    "    sns.heatmap(pivot_table, cmap='viridis', annot=False, fmt=\".0f\", linewidths=.5, linecolor='lightgray')\n",
    "    plt.title(f\"4-2b Average {target_col} Heatmap: Hour of Day vs. Day of Week ({router_label})\")\n",
    "    plt.xlabel(\"Day of Week\")\n",
    "    plt.ylabel(\"Hour of Day\")\n",
    "    save_plot(f\"4_2b_weekday_heatmap\", router_label)\n",
    "\n",
    "    # Anomaly Detection Plots (Figures 4-3a/b/c)\n",
    "    if ANOM_SCORE_COL in df_hourly_copy.columns and ANOM_FLAG_COL in df_hourly_copy.columns:\n",
    "        print(f\"\\n--- Generating Anomaly Detection Plots for {router_label} ---\")\n",
    "        # Fig 4-3a: Isolation-Forest score histogram\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sorted_scores = np.sort(df_hourly_copy[ANOM_SCORE_COL].values)\n",
    "        threshold_idx = int((1 - IF_CONTAMINATION) * len(sorted_scores))\n",
    "        threshold_score = sorted_scores[threshold_idx]\n",
    "        sns.histplot(df_hourly_copy[ANOM_SCORE_COL], bins=100, kde=True, color='teal', alpha=0.7)\n",
    "        plt.axvline(x=threshold_score, color='red', linestyle='--', label=f'Threshold (Contamination={IF_CONTAMINATION})')\n",
    "        plt.title(f\"4-3a Isolation Forest Anomaly Score Histogram ({router_label})\")\n",
    "        plt.xlabel(\"Anomaly Score (Higher = More Anomalous)\")\n",
    "        plt.ylabel(\"Number of Observations\")\n",
    "        plt.legend()\n",
    "        save_plot(f\"4_3a_if_score_histogram\", router_label)\n",
    "\n",
    "        # Fig 4-3b: Time-series plot with anomalies marked\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.plot(df_hourly_copy.index, df_hourly_copy[target_col], label='Original Data', color='blue', alpha=0.7)\n",
    "        anomalies = df_hourly_copy[df_hourly_copy[ANOM_FLAG_COL] == 1]\n",
    "        plt.scatter(anomalies.index, anomalies[target_col], color='red', s=50, label='Detected Anomaly', zorder=5)\n",
    "        plt.title(f\"4-3b {target_col} Time Series with Detected Anomalies ({router_label})\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(f\"{target_col}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle=':', alpha=0.7)\n",
    "        save_plot(f\"4_3b_timeseries_anomalies\", router_label)\n",
    "\n",
    "        # Fig 4-3c: Anomaly count bar-chart by day of week\n",
    "        anomaly_counts_per_day = df_hourly_copy[df_hourly_copy[ANOM_FLAG_COL] == 1]['day_name'].value_counts().reindex(day_order).fillna(0)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=anomaly_counts_per_day.index, y=anomaly_counts_per_day.values, palette='viridis')\n",
    "        plt.title(f\"4-3c Anomaly Counts by Day of Week ({router_label})\")\n",
    "        plt.xlabel(\"Day of Week\")\n",
    "        plt.ylabel(\"Number of Anomalies\")\n",
    "        plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "        save_plot(f\"4_3c_anomaly_counts_by_day\", router_label)\n",
    "    else:\n",
    "        print(f\"  Anomaly columns not found in df_hourly for {router_label}. Skipping anomaly plots.\")\n",
    "\n",
    "def plot_global_eda(all_hourly_mean_per_hour: pd.DataFrame, all_raw_samples_target_col: pd.Series, target_col: str):\n",
    "    \"\"\"\n",
    "    Generates global (cross-router) EDA plots (Fig 4-2a, 4-2c).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Global EDA Plots (Figures 4-2a, 4-2c) ---\")\n",
    "\n",
    "    # Fig 4-2a: 10-router daily curves (mean traffic per hour)\n",
    "    if not all_hourly_mean_per_hour.empty: # Only plot if data is provided\n",
    "        print(\"  Generating Figure 4-2a (10-Router Daily Curves)...\")\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        sns.lineplot(x='hour_of_day', y='mean_packets', hue='router', data=all_hourly_mean_per_hour, palette='tab10')\n",
    "        plt.title(f\"4-2a Daily Utilization Curves (Mean {target_col} per Hour Across Routers)\")\n",
    "        plt.xlabel(\"Hour of Day (0-23)\")\n",
    "        plt.ylabel(f\"Mean {target_col}\")\n",
    "        plt.xticks(range(24))\n",
    "        plt.legend(title='Router', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, linestyle=':', alpha=0.7)\n",
    "        save_plot(f\"4_2a_daily_curves_all_routers\", router_label=\"\")\n",
    "    else:\n",
    "        print(\"  No hourly mean data collected. Skipping Fig 4-2a.\")\n",
    "\n",
    "    # Fig 4-2c: Flow-size distribution (log-histogram of raw packets)\n",
    "    if not all_raw_samples_target_col.empty: # Only plot if data is provided\n",
    "        print(\"  Generating Figure 4-2c (Flow Size Distribution - All Routers Sample)...\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(np.log1p(all_raw_samples_target_col), bins=50, kde=True, color='purple', alpha=0.7)\n",
    "        plt.title(f\"4-2c Log-Histogram of Flow Sizes (All Routers Sample)\")\n",
    "        plt.xlabel(f\"log(1 + {target_col})\")\n",
    "        plt.ylabel(\"Number of Flows (Count)\")\n",
    "        plt.grid(True, linestyle=':', alpha=0.7)\n",
    "        save_plot(f\"4_2c_flow_size_histogram_all_routers\", router_label=\"\")\n",
    "    else:\n",
    "        print(f\"  No raw samples data for flow-size histogram. Skipping Fig 4-2c.\")\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# Anomaly Detection (Isolation Forest)                                #\n",
    "#######################################################################\n",
    "\n",
    "def run_isolation_forest(series: pd.Series, contamination: float, router_label: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fits Isolation Forest on the time series and returns anomaly scores and flags.\n",
    "    The input `series` should be the hourly aggregated data (e.g., in_packets).\n",
    "    \"\"\"\n",
    "    print(f\"  Running Isolation Forest for Anomaly Detection for {router_label}...\")\n",
    "    \n",
    "    if series.empty:\n",
    "        print(f\"  Warning: Input series for Isolation Forest is empty for {router_label}. Skipping.\")\n",
    "        return pd.DataFrame(columns=[series.name, ANOM_SCORE_COL, ANOM_FLAG_COL])\n",
    "\n",
    "    X = series.values.reshape(-1, 1)\n",
    "    \n",
    "    iforest = IsolationForest(n_estimators=200, contamination=contamination, random_state=SEED, n_jobs=-1) \n",
    "    iforest.fit(X)\n",
    "    \n",
    "    scores = -iforest.decision_function(X)\n",
    "    flags = iforest.predict(X)\n",
    "    flags = np.where(flags == -1, 1, 0)\n",
    "    \n",
    "    out_df = pd.DataFrame({\n",
    "        series.name: series.values,\n",
    "        ANOM_SCORE_COL: scores,\n",
    "        ANOM_FLAG_COL: flags\n",
    "    }, index=series.index)\n",
    "    \n",
    "    print(f\"  Isolation Forest detection complete for {router_label}. Found {out_df[ANOM_FLAG_COL].sum()} anomalies.\")\n",
    "    return out_df\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# Main Execution for Notebook 1                                       #\n",
    "#######################################################################\n",
    "\n",
    "def main():\n",
    "    print(\"\\n--- Initiating Notebook 1: Raw Data ETL & Anomaly Flagging (Multi-Router) ---\")\n",
    "\n",
    "    # 1. Get all individual router Parquet file paths\n",
    "    all_router_file_paths = sorted(list(INPUT_ROUTERS_DIR.glob(\"*.parquet\")))\n",
    "    if not all_router_file_paths:\n",
    "        print(f\"Error: No .parquet files found in {INPUT_ROUTERS_DIR}. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFound {len(all_router_file_paths)} individual router Parquet files in {INPUT_ROUTERS_DIR}.\")\n",
    "\n",
    "    # Data structures to collect info for global EDA plots and Table 4-1\n",
    "    router_inventory_data = [] \n",
    "    all_hourly_mean_per_hour_list = [] \n",
    "    all_raw_samples_target_col_list = [] # For Fig 4-2c (Flow-size Distribution)\n",
    "\n",
    "    # Process each router file sequentially\n",
    "    for file_path_idx, file_path in enumerate(tqdm(all_router_file_paths, desc=\"Processing Routers\")):\n",
    "        router_label = file_path.stem # Extract router name from filename (e.g., \"atlanta\")\n",
    "        print(f\"\\n==== Starting Full Processing for Router: {router_label} ({file_path_idx + 1}/{len(all_router_file_paths)}) ====\")\n",
    "        \n",
    "        # 1.1. Load Individual Raw File (Dask, with column selection)\n",
    "        print(f\"  Loading raw data for {router_label} from {file_path.name} with selected columns...\")\n",
    "        try:\n",
    "            # Load only necessary columns for initial processing and memory efficiency\n",
    "            # Ensure 't_last' is also loaded if it's used in initial cleaning (e.g., for flow duration analysis, though not in this pipeline)\n",
    "            ddf_raw_single_router = dd.read_parquet(file_path, engine=\"pyarrow\", columns=[TIMESTAMP_COL, TARGET_COL] + ADDITIONAL_RAW_COLS)\n",
    "            # Ensure router column is present and correctly typed (though it's derived later)\n",
    "            ddf_raw_single_router[ROUTER_COL] = router_label\n",
    "            print(f\"  Loaded Dask DataFrame (lazy) for {router_label}. Partitions: {ddf_raw_single_router.npartitions}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading Dask DataFrame for {router_label}: {e}. Skipping this router.\")\n",
    "            continue # Skip to next router\n",
    "\n",
    "        # 1.2. Collect raw data stats for Table 4-1 (Dask-native count)\n",
    "        num_flows_raw = ddf_raw_single_router.shape[0].compute() \n",
    "        print(f\"  Raw data for {router_label}: {num_flows_raw} records.\")\n",
    "\n",
    "        # 1.3. Collect a sample of raw 'in_packets' for the global histogram (Fig 4-2c)\n",
    "        if num_flows_raw > 0 and TARGET_COL in ddf_raw_single_router.columns:\n",
    "            # Dask's .sample(frac=...) is memory-efficient.\n",
    "            # Using a fixed fraction across routers to ensure representation.\n",
    "            raw_sample_per_router = ddf_raw_single_router[TARGET_COL].sample(frac=min(1.0, GLOBAL_HISTOGRAM_SAMPLE_PER_ROUTER_COUNT / num_flows_raw), random_state=SEED).compute()\n",
    "            all_raw_samples_target_col_list.append(raw_sample_per_router)\n",
    "            print(f\"  Collected {len(raw_sample_per_router)} raw samples for global histogram from {router_label}.\")\n",
    "        else:\n",
    "            print(f\"  No raw data or target column '{TARGET_COL}' found for {router_label}. Skipping raw sample collection for histogram.\")\n",
    "\n",
    "        # 2. Dask-native Initial Clean and Hourly Resampling\n",
    "        print(f\"  Performing Dask-native initial cleaning and hourly resampling for {router_label}...\")\n",
    "        \n",
    "        # Convert timestamps to datetime in Dask and drop NaNs\n",
    "        ddf_cleaned_and_filtered = ddf_raw_single_router.copy()\n",
    "        ddf_cleaned_and_filtered[TIMESTAMP_COL] = dd.to_datetime(ddf_cleaned_and_filtered[TIMESTAMP_COL], errors='coerce')\n",
    "        ddf_cleaned_and_filtered = ddf_cleaned_and_filtered.dropna(subset=[TIMESTAMP_COL])\n",
    "\n",
    "        if ddf_cleaned_and_filtered.npartitions == 0 or ddf_cleaned_and_filtered.shape[0].compute() == 0:\n",
    "            print(f\"  Warning: No valid records after initial cleaning for {router_label}. Skipping further processing for this router.\")\n",
    "            num_hourly_rows = 0\n",
    "            router_inventory_data.append({\n",
    "                \"Router ID\": router_label,\n",
    "                \"Raw Flows (#)\": num_flows_raw,\n",
    "                \"Hourly Rows (#)\": num_hourly_rows\n",
    "            })\n",
    "            del ddf_raw_single_router, ddf_cleaned_and_filtered\n",
    "            gc.collect()\n",
    "            continue\n",
    "\n",
    "        # Set index and resample (still Dask)\n",
    "        # Note: If timestamps are not already sorted, this can be slow or incorrect.\n",
    "        # `sorted=True` is a hint to Dask.\n",
    "        ddf_hourly_router = ddf_cleaned_and_filtered.set_index(TIMESTAMP_COL, sorted=True)[TARGET_COL].resample(FREQ).sum()\n",
    "        \n",
    "        # Convert back to DataFrame and add router column (still Dask)\n",
    "        ddf_hourly_router = ddf_hourly_router.to_frame()\n",
    "        ddf_hourly_router[ROUTER_COL] = router_label\n",
    "\n",
    "        # 3. Compute Hourly Aggregated Data to Pandas (Memory-Safe)\n",
    "        # This is the crucial step: df_hourly_resampled should be small (max ~1368 rows).\n",
    "        df_hourly_resampled = ddf_hourly_router.compute() \n",
    "        print(f\"  Computed hourly data for {router_label}: {len(df_hourly_resampled)} rows.\")\n",
    "        # Explicitly delete Dask DataFrames to free up graph memory\n",
    "        del ddf_raw_single_router, ddf_cleaned_and_filtered, ddf_hourly_router \n",
    "        gc.collect()\n",
    "\n",
    "        # 4. Pandas Post-Resampling (Interpolation/Fill)\n",
    "        if df_hourly_resampled.empty:\n",
    "            print(f\"  Warning: Hourly resampled data for {router_label} is empty after Dask compute. Skipping further processing for this router.\")\n",
    "            num_hourly_rows = 0\n",
    "            router_inventory_data.append({\n",
    "                \"Router ID\": router_label,\n",
    "                \"Raw Flows (#)\": num_flows_raw,\n",
    "                \"Hourly Rows (#)\": num_hourly_rows\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        missing_before_interp = df_hourly_resampled[TARGET_COL].isnull().sum()\n",
    "        df_hourly_resampled[TARGET_COL] = df_hourly_resampled[TARGET_COL].interpolate(method='linear')\n",
    "        df_hourly_resampled[TARGET_COL].fillna(0, inplace=True)\n",
    "        if missing_before_interp > 0:\n",
    "            print(f\"  Interpolated {missing_before_interp} missing values linearly for hourly data.\")\n",
    "        \n",
    "        # Collect hourly data stats for Table 4-1\n",
    "        num_hourly_rows = len(df_hourly_resampled)\n",
    "        router_inventory_data.append({\n",
    "            \"Router ID\": router_label,\n",
    "            \"Raw Flows (#)\": num_flows_raw,\n",
    "            \"Hourly Rows (#)\": num_hourly_rows\n",
    "        })\n",
    "\n",
    "        # 5. Run Anomaly Detection (Isolation Forest) (on Pandas df_hourly_resampled)\n",
    "        df_hourly_with_anomalies = run_isolation_forest(\n",
    "            df_hourly_resampled[TARGET_COL], IF_CONTAMINATION, router_label\n",
    "        )\n",
    "        # Add router column back, ensuring it's consistently set if it was lost in resampling.\n",
    "        df_hourly_with_anomalies[ROUTER_COL] = router_label \n",
    "        del df_hourly_resampled \n",
    "        gc.collect()\n",
    "\n",
    "        # 6. Save Processed Hourly Data for this router\n",
    "        processed_hourly_parquet_path = OUT_DIR / f\"hourly_{router_label.lower().replace(' ', '_')}_processed_with_anomalies.parquet\"\n",
    "        print(f\"  Saving processed hourly data for {router_label} to: {processed_hourly_parquet_path}\")\n",
    "        df_hourly_with_anomalies.to_parquet(processed_hourly_parquet_path, index=True) \n",
    "        print(f\"  Processed hourly data for {router_label} saved successfully.\")\n",
    "\n",
    "        # 7. Collect data for global Fig 4-2a (Daily Utilization Curves)\n",
    "        hourly_mean_per_hour = df_hourly_with_anomalies.groupby(df_hourly_with_anomalies.index.hour)[TARGET_COL].mean().reset_index()\n",
    "        hourly_mean_per_hour.columns = ['hour_of_day', 'mean_packets']\n",
    "        hourly_mean_per_hour[ROUTER_COL] = router_label \n",
    "        all_hourly_mean_per_hour_list.append(hourly_mean_per_hour)\n",
    "\n",
    "        # 8. Generate Illustrative EDA Plots (Single Router: 4-2b, 4-3a/b/c)\n",
    "        # Choose \"elpaso\" as the illustrative router\n",
    "        if router_label == \"elpaso\": \n",
    "            plot_eda_single_router(df_hourly_with_anomalies.copy(), router_label, TARGET_COL)\n",
    "        else:\n",
    "            print(f\"  Skipping single-router EDA plots for {router_label} (generating for El Paso as representative).\")\n",
    "\n",
    "        del df_hourly_with_anomalies \n",
    "        gc.collect()\n",
    "\n",
    "    # After processing all routers, plot global EDA\n",
    "    print(\"\\n==== All Routers Processed. Generating Global EDA Plots ====\")\n",
    "    \n",
    "    # Fig 4-2a: 10-router daily curves\n",
    "    if all_hourly_mean_per_hour_list:\n",
    "        combined_hourly_mean_per_hour_df = pd.concat(all_hourly_mean_per_hour_list, ignore_index=True)\n",
    "        # Pass empty Series for raw samples as they are handled by a separate list\n",
    "        plot_global_eda(combined_hourly_mean_per_hour_df, pd.Series(dtype=float), TARGET_COL) \n",
    "    else:\n",
    "        print(\"  No hourly mean data collected. Skipping Fig 4-2a.\")\n",
    "    \n",
    "    # Fig 4-2c: Flow-size distribution\n",
    "    if all_raw_samples_target_col_list:\n",
    "        final_all_raw_samples_series = pd.concat(all_raw_samples_target_col_list, ignore_index=True)\n",
    "        plot_global_eda(pd.DataFrame(), final_all_raw_samples_series, TARGET_COL) \n",
    "    else:\n",
    "        print(\"  No raw samples collected. Skipping Fig 4-2c.\")\n",
    "\n",
    "    # Print Table 4-1: Router Inventory\n",
    "    print(\"\\n--- Table 4-1: Router Inventory (Raw Data & Hourly Aggregates) ---\")\n",
    "    router_inventory_df = pd.DataFrame(router_inventory_data)\n",
    "    inventory_cols = [\"Router ID\", \"Raw Flows (#)\", \"Hourly Rows (#)\"]\n",
    "    router_inventory_df = router_inventory_df[inventory_cols] \n",
    "    print(router_inventory_df.to_string())\n",
    "    \n",
    "    print(\"\\n--- Notebook 1 Complete: Raw Data ETL & Anomaly Flagging ---\")\n",
    "    print(f\"Check the '{OUT_DIR}' directory for processed hourly Parquet files (e.g., hourly_atlanta_processed_with_anomalies.parquet).\")\n",
    "    print(f\"Check the '{FIG_DIR}' directory for generated figures.\")\n",
    "\n",
    "# Execute the main pipeline for this notebook\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mahin's Thesis Venv",
   "language": "python",
   "name": "mahin_mthesis_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
